{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk-E_yczjk4o"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "max_seq_length = 4096\n",
        "dtype = None # None for auto detection\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage\n",
        "BATCH_SIZE = 48\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "z2Hfa5zTjoQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_CSV = \"steam_learning_corpus_full_context.csv\"\n",
        "OUTPUT_JSON = \"annotated_learning_corpus.json\"\n",
        "OUTPUT_CSV = \"annotated_learning_corpus_final.csv\""
      ],
      "metadata": {
        "id": "CIJap_yN3Xkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. PROMPT TEMPLATES (ROUTING LOGIC)\n",
        "\n",
        "# PROMPT A: For rows where 'has_perception' == True (contains words like see, hear, notice)\n",
        "prompt_perception = f\"\"\"<|im_start|>system\n",
        "# Role\n",
        "You are an expert qualitative data analyst specializing in game studies and educational psychology. Your task is to analyze a video game review to determine if a specific highlighted verb indicates a genuine instance of \"perceived learning.\"\n",
        "\n",
        "# Input Format\n",
        "You will receive a Steam review. A specific verb will be highlighted in double brackets, like this: \"I [[realized]] that the factory must grow.\"\n",
        "\n",
        "# Analysis Guidelines\n",
        "\n",
        "1. **Verify Genuine Learning (True/False)**\n",
        "   Determine if the highlighted verb describes the player acquiring new skills, knowledge, understanding, or insight *within the context of playing the game*.\n",
        "   * **TRUE Examples:**\n",
        "       * \"I [[learned]] how to optimize my train network.\"\n",
        "       * \"We [[figured]] out the solution to the puzzle.\"\n",
        "       * \"I [[realized]] the story was a metaphor for grief.\"\n",
        "   * **FALSE Examples:**\n",
        "       * \"I [[learned]] about this game from a friend.\"\n",
        "       * \"I [[learned]] my lesson: never buy early access.\"\n",
        "       * \"I [[guess]] it's okay.\"\n",
        "\n",
        "2. **Categorize the Learning**\n",
        "   If Genuine Learning is TRUE, categorize it into one of the following:\n",
        "   * **SYSTEMS:** Learning mechanics, controls, optimization, physics, or game rules (e.g., automation in Factorio, puzzles in Portal.) Example: \"I kept dying to the boss until I [[realized]] that electricity damage stuns him for a few seconds.\"\n",
        "   * **NARRATIVE:** Learning plot points, lore, character backstories, or thematic meaning. Example: \"By reading the terminal logs, we [[discovered]] that the corporation had been poisoning the planet long before the aliens arrived.\"\n",
        "   * **SOCIAL_SELF:** Learning about teamwork, coordination, leadership, patience, or personal capability (common in Co-op). Example: \"We kept failing the timer, so we [[figured]] out that one of us needed to just call out commands while the other two executed them.\"\n",
        "   * **OTHER:** If none of the previous categories fit.\n",
        "   * **NONE:** Use this if Genuine Learning is False.\n",
        "\n",
        "3. **Verify Perceptual Link (True/False)**\n",
        "   Is the learning *directly caused by* or *framed through* an act of perception mentioned in the text?\n",
        "   * **TRUE:** The player learned *because* they saw/heard/noticed something.\n",
        "       * *Example:* \"I saw the blinking light and [[realized]] the battery was low.\" (Seeing caused the realization).\n",
        "       * *Example:* \"I noticed the pattern and [[figured]] it out.\"\n",
        "   * **FALSE:** Perception words are present but unrelated to the specific learning event.\n",
        "       * *Example:* \"The game looks great. I also [[learned]] how to jump.\" (Visuals and learning are separate).\n",
        "\n",
        "# Output Format\n",
        "Output ONLY a valid JSON object with the following schema:\n",
        "\n",
        "{{\n",
        "  \"is_genuine_learning\": boolean,\n",
        "  \"learning_category\": \"SYSTEMS\" | \"NARRATIVE\" | \"SOCIAL_SELF\" | \"OTHER\" | \"NONE\",\n",
        "  \"perception_linked_to_learning\": boolean,\n",
        "}}\n",
        "<|im_end|>\"\"\"\n",
        "\n",
        "# PROMPT B: For rows where 'has_perception' == False (words like realize, figure out, understand)\n",
        "prompt_cognition = f\"\"\"<|im_start|>system\n",
        "# Role\n",
        "You are an expert qualitative data analyst specializing in game studies and educational psychology. Your task is to analyze a video game review to determine if a specific highlighted verb indicates a genuine instance of \"perceived learning.\"\n",
        "\n",
        "# Input Format\n",
        "You will receive a Steam review. A specific verb will be highlighted in double brackets, like this: \"I [[realized]] that the factory must grow.\"\n",
        "\n",
        "# Analysis Guidelines\n",
        "\n",
        "1. **Verify Genuine Learning (True/False)**\n",
        "   Determine if the highlighted verb describes the player acquiring new skills, knowledge, understanding, or insight *within the context of playing the game*.\n",
        "   * **TRUE Examples:**\n",
        "       * \"I [[learned]] how to optimize my train network.\"\n",
        "       * \"We [[figured]] out the solution to the puzzle.\"\n",
        "       * \"I [[realized]] the story was a metaphor for grief.\"\n",
        "   * **FALSE Examples:**\n",
        "       * \"I [[learned]] about this game from a friend.\"\n",
        "       * \"I [[learned]] my lesson: never buy early access.\"\n",
        "       * \"I [[guess]] it's okay.\"\n",
        "\n",
        "2. **Categorize the Learning**\n",
        "   If Genuine Learning is TRUE, categorize it into one of the following:\n",
        "   * **SYSTEMS:** Learning mechanics, controls, optimization, physics, or game rules (e.g., automation in Factorio, puzzles in Portal.) Example: \"I kept dying to the boss until I [[realized]] that electricity damage stuns him for a few seconds.\"\n",
        "   * **NARRATIVE:** Learning plot points, lore, character backstories, or thematic meaning. Example: \"By reading the terminal logs, we [[discovered]] that the corporation had been poisoning the planet long before the aliens arrived.\"\n",
        "   * **SOCIAL_SELF:** Learning about teamwork, coordination, leadership, patience, or personal capability (common in Co-op). Example: \"We kept failing the timer, so we [[figured]] out that one of us needed to just call out commands while the other two executed them.\"\n",
        "   * **OTHER:** If none of the previous categories fit.\n",
        "   * **NONE:** Use this if Genuine Learning is False.\n",
        "\n",
        "# Output Format\n",
        "Output ONLY a valid JSON object with the following schema:\n",
        "\n",
        "{{\n",
        "  \"is_genuine_learning\": boolean,\n",
        "  \"learning_category\": \"SYSTEMS\" | \"NARRATIVE\" | \"SOCIAL_SELF\" | \"OTHER\" | \"NONE\"\n",
        "}}\n",
        "<|im_end|>\"\"\"\n",
        "\n",
        "def create_prompt(review_text, has_perception):\n",
        "    # Select the system prompt based on the boolean flag\n",
        "    sys_prompt = prompt_perception if has_perception else prompt_cognition\n",
        "\n",
        "    return f\"\"\"{sys_prompt}\n",
        "<|im_start|>user\n",
        "Review:\n",
        "{review_text}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MQC9h3wTjtLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. DATA LOADING\n",
        "print(f\"Loading data from {INPUT_CSV}...\")\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_CSV)\n",
        "    # Filter for valid inputs\n",
        "    df = df.dropna(subset=['full_review_highlighted'])\n",
        "    print(f\"Loaded {len(df)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    # Create dummy data for testing if file missing\n",
        "    df = pd.DataFrame([{\n",
        "        'full_review_highlighted': \"I looked at the water and [[realized]] that I could swim underneath the base.\",\n",
        "        'learning_verb': 'realized',\n",
        "        'game': 'Subnautica'\n",
        "    }])\n",
        "    print(\"Created dummy data for testing.\")\n",
        "\n",
        "# Convert to list of dicts for processing\n",
        "data_records = df.to_dict('records')"
      ],
      "metadata": {
        "id": "xiMrxsb03x3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. TEST RUN (Single Example)\n",
        "print(\"\\n--- Running Test Example ---\")\n",
        "test_record = data_records[0]\n",
        "has_perception = test_record.get('has_perception', False)\n",
        "test_prompt = create_prompt(test_record['full_review_highlighted'], has_perception)\n",
        "\n",
        "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1024,\n",
        "    use_cache=True,\n",
        "    temperature = 0.6, top_p = 0.95, top_k = 20, # Low temperature for consistent JSON\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "decoded_test = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "print(\"Input Snippet:\", test_record['full_review_highlighted'][:100] + \"...\")\n",
        "print(\"Model Output:\", decoded_test)\n",
        "\n",
        "# Verify JSON parsing\n",
        "try:\n",
        "    json.loads(decoded_test)\n",
        "    print(\"✅ JSON Parsing Successful\")\n",
        "except:\n",
        "    print(\"❌ JSON Parsing Failed (Will use regex in batch loop)\")"
      ],
      "metadata": {
        "id": "yT7c_G5y31AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. BATCH PROCESSING\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"\\nStarting batch processing of {len(data_records)} records...\")\n",
        "annotated_records = []\n",
        "batch_queue = []\n",
        "\n",
        "# Prepare prompts\n",
        "for i, record in enumerate(data_records):\n",
        "    # FIX: Pass the 'has_perception' flag from the CSV record!\n",
        "    # Ensure your CSV actually has this column as a boolean\n",
        "    has_perception = record.get('has_perception', False)\n",
        "\n",
        "    batch_queue.append({\n",
        "        \"original_index\": i,\n",
        "        \"prompt\": create_prompt(record['full_review_highlighted'], has_perception)\n",
        "    })\n",
        "\n",
        "# Process in chunks\n",
        "for i in tqdm(range(0, len(batch_queue), BATCH_SIZE), desc=\"Annotating\"):\n",
        "    batch = batch_queue[i : i + BATCH_SIZE]\n",
        "    prompts = [item[\"prompt\"] for item in batch]\n",
        "\n",
        "    # Tokenize (Left padding is often safer for generation, but unsloth handles it well)\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2048, # 2048 is likely overkill for a JSON response, 512 is faster\n",
        "            temperature = 0.6, top_p = 0.95, top_k = 20, # Lower temp is better for strict JSON\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_texts = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    # Parse and Merge Results\n",
        "    for j, gen_text in enumerate(generated_texts):\n",
        "        idx = batch[j][\"original_index\"]\n",
        "        original_record = data_records[idx].copy()\n",
        "\n",
        "        try:\n",
        "            # Regex to find JSON block\n",
        "            json_match = re.search(r'\\{.*\\}', gen_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                parsed_json = json.loads(json_match.group(0))\n",
        "            else:\n",
        "                parsed_json = json.loads(gen_text)\n",
        "\n",
        "            # FIX: Mapping keys correctly based on PROMPT OUTPUT SCHEMA\n",
        "            original_record['llm_is_genuine'] = parsed_json.get('is_genuine_learning', False)\n",
        "            original_record['llm_category'] = parsed_json.get('learning_category', 'NONE')\n",
        "            original_record['llm_perception_linked'] = parsed_json.get('perception_linked_to_learning', False)\n",
        "            original_record['llm_reasoning'] = parsed_json.get('reasoning', '')\n",
        "\n",
        "        except Exception as e:\n",
        "            original_record['llm_error'] = str(e)\n",
        "            original_record['raw_llm_output'] = gen_text\n",
        "\n",
        "        annotated_records.append(original_record)"
      ],
      "metadata": {
        "id": "2Q8uYB1j37TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. SAVE RESULTS\n",
        "print(f\"\\nSaving results to {OUTPUT_CSV}...\")\n",
        "final_df = pd.DataFrame(annotated_records)\n",
        "final_df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "# Optional: Save JSON version\n",
        "with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
        "    json.dump(annotated_records, f, indent=2)\n",
        "\n",
        "print(\"Processing complete. Download your files from the file browser.\")\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "files_to_download = [\"annotated_learning_corpus.json\", \"annotated_learning_corpus_final.csv\"]\n",
        "\n",
        "for filename in files_to_download:\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        files.download(filename)\n",
        "    else:\n",
        "        print(f\"File not found: {filename}. Did you run the previous cells?\")"
      ],
      "metadata": {
        "id": "TjM5w1L64I-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}